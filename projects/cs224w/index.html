<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Protein Function Prediction using LoRA-Relational Graph Neural Networks and Multi-Task Learning | Sergio Charles</title> <meta name="author" content="Sergio Charles"> <meta name="description" content="Using multi-task and relational graph neural networks to predict protein function."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/sigma_img.png"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sergiogcharles.github.io/projects/cs224w/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Sergio </span><span class="font-weight-bold">Charles </span></a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">research</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Protein Function Prediction using LoRA-Relational Graph Neural Networks and Multi-Task Learning</h1> <p class="post-description">Using multi-task and relational graph neural networks to predict protein function.</p> </header> <article> <p>This is my class project for CS224W: Machine Learning on Graphs, done in collaboration with Alec Lessing and Matt Wolff. I won’t go into every detail here, but you can <a href="https://medium.com/@sergiogcharles/funce-gnn-protein-function-prediction-using-multi-task-and-relational-graph-learning-74f972a776f9" rel="external nofollow noopener" target="_blank">read our in-depth Medium post if you want the full picture!</a></p> <p>The task was to predict whether or not a protein possesed a particular functional property. We used AlphaFold 2’s protein structure predictions. Each node in the input graph represents an atom with features consisting of the 3D atom coordinates and the one-hot-encoded atom type. We also augmented these features with cycle counts, in order to surpass the expressivity limitations of the Weisfeiler Leman (WL) isomorphism test.</p> <h2 id="structural-feature-augmentation">Structural Feature Augmentation</h2> <p>More precisely, in the naive setting, the feature for node \(i\) is its one-hot-encoded atom type, a number representing the number of protons in the atom. In our dataset, the only atom types are Boron, Carbon, Nitrogen, Sulfur so we use a 4-dimensional one-hot encoding for atom type. To make the graph neural network (GNN) more expressive, the feature for node \(i\) will be augmented by:</p> \[\mathbf{s}=\begin{bmatrix} diag(A^0)_{i,i} ,\dots, diag(A^{K-1})_{i,i} \end{bmatrix}\in\mathbb{R}^K\] <p>where the \((i,i)\) entry of the diagonal of the \(k\)-th power of the adjacency matrix \(A\) corresponds to the number of \(k\)-length cycles node \(i\) resides in. In fact, <a href="https://ieeexplore.ieee.org/document/10447704" rel="external nofollow noopener" target="_blank">in the recent work of Kanatsoulis et al.</a>, it was shown that a Graph Isomorphism Network (the most powerful GNN in the class of message-passing GNNs) with such structural initial node features is strictly more powerful than the WL test!</p> <h2 id="three-way-classification">Three-way Classification</h2> <p>We use structural counts, atom types, and coordinates, to make a graph-level 3-way classification of whether a protein (1) contributes to, (2) enables, or (3) does not enable/contribute to a specific function.</p> <h2 id="heterogeneous-protein-graph-and-relational-gat">Heterogeneous Protein Graph and Relational GAT</h2> <p>We obtain a graph-level embedding for the protein using a graph neural network:</p> \[\begin{split} &amp;f_{\theta}: \mathbb{R}^{N\times (K+4)} \times \mathbb{R}^{N\times 3} \rightarrow \mathbb{R}^H\\ &amp;\mathbf{p}_{\theta}=f_{\theta}(H, X) \end{split}\] <p>where \(H\in\mathbb{R}^{N\times (K + 4)}\) is the matrix of augmented node features and \(X\in\mathbb{R}^{N\times 3}\) is the matrix of the predicted coordinates for the protein from AlphaFold.</p> <p>We view the protein graph as a heterogenous graph, whereby (source_atom_type, target_atom_type) is an edge type. In our setting, this is equivalent to a relation type. For instance, in the glycine amino acid molecule shown below, (Carbon, Hydrogen) is distinct from (Carbon, Nitrogen). Intuitively, we should treat the graph as heterogenous so the message-passing GNN can distinguish edge types. In our dataset, there are 16 unique edge types.</p> <p><img src="/assets/img/hetero_graph.png" width="700px"></p> <p>As shown in this figure, if we we update the feature representation for the central Carbon atom, we would only perform message passing and aggregation over distinct edge types, namely $e_1$: (C, N), $e_2$: (C, H), $e_3$: (C, C). We use <a href="https://arxiv.org/abs/1904.05811" rel="external nofollow noopener" target="_blank">Relational Graph Attention Network (RGAT)</a> to process this heterogenous protein graph structure. We apply a linear transformation to the input features for each relation type \(r\):</p> \[\begin{split} &amp;H\in\mathbb{R}^{N\times F}\\ &amp;W_1^{(r)}\in\mathbb{R}^{F\times F'}\\ &amp;G^{(r)}=HW_1^{(r)}=\begin{bmatrix}\mathbf{g}_1^{(r)} &amp; \dots &amp; \mathbf{g}_N^{(r)} \end{bmatrix}\in\mathbb{R}^{N\times F'} \end{split}\] <p>Given linear transformations \(W_1^{(r)}\) of type \(r\), we assume that the logits of each relation are independent, given by:</p> \[E_{ij}^{(r)}=a(\mathbf{g}_i^{(r)}, \mathbf{g}_j^{(r)}, \mathbf{e}_{ij}^{(r)}).\] <p>This denotes the importance of node \(j\)’s representation to that of node \(i\) under relation \(r\). We additionally condition on the edge attribute \(\mathbf{e}_{ij}\), which we will define as the distance between atoms in our application. We chose to use an additive attention mechanism a, whereby we project intermediate representations \(g\) into query and key representations of dimension \(D\) and add them with the projected pair-wise distance edge attribute:</p> \[\begin{split} &amp;Q^{(r)}, K^{(r)}\in\mathbb{R}^{F'\times D}, W_2^{(r)}\in\mathbb{R}^{D\times 1}\\ &amp;\mathbf{q}_i^{(r)}=g_i^{(r)}Q^{(r)}\in\mathbb{R}^D\\ &amp;\mathbf{k}_i^{(r)}=g_i^{(r)}K^{(r)}\in\mathbb{R}^D\\ &amp;\mathbf{e}_{ij}^{(r)}=||\mathbf{x}_i^{(r)}-\mathbf{x}_j^{(r)}||_2\in\mathbb{R}\\ &amp;E_{ij}^{(r)}=\text{LeakReLU}(q_i^{(r)} + k_j^{(r)} + W_2^{(r)}\mathbf{e}_{ij}^{(r)}). \end{split}\] <p>Then, attention coefficients are computed across neighborhoods, irrespective of the relation - this is known as \(\textit{Across-Relation Graph Attention}\):</p> \[\begin{split} &amp;\alpha_{i,j}^{(r)}=\text{softmax}_{j,r}(E_{ij}^{(r)})=\frac{\exp(E_{ij}^{(r)})}{\sum_{r'\in\mathcal{R}}\sum_{k\in\mathcal{N}_i^{(r')}}\exp(E_{ik}^{(r')})}\\ &amp;\sum_{r\in\mathcal{R}}\sum_{j\in\mathcal{N}_i^{(r)}}\alpha_{i,j}^{(r)}=1 \end{split}\] <p>where \(\mathcal{N}_i^{(r)}\) represents the neighborhood of \(i\) under relation \(r\). Finally, we aggregate the intermediate representations and apply a non-linear activation:</p> \[\mathbf{h}_i'=\sigma\left(\sum_{r\in\mathcal{R}}\sum_{j\in\mathcal{N}_i^{(r)}}\alpha_{ij}^{(r)}\mathbf{g}_j^{(r)}\right)\in\mathbb{R}^{N\times F'}.\] <h2 id="low-rank-adaptation-for-efficient-relational-graph-neural-networks">Low-Rank Adaptation for Efficient Relational Graph Neural Networks</h2> <p>As we have 16 relation types and use a large hidden dimension in our experiments (512), it would be inefficient to maintain a weight matrix W(r) for every type, as described in RGAT. Thus, we propose a decomposition based on Low-Rank Adaptation of Large Language Models [12]. We decomposed W(r) into a shared weight matrix for all relation types and a low-rank decomposition for the relation-specific component. That is: \(W_r=W_{\text{shared}} + A_r B_r\) where \(W_{\text{shared}}\in\mathbb{R}^{F\times F'}\) is a shared weight matrix and \(A_r\in\mathbb{R}^{F\times r}\), \(B_r\in\mathbb{R}^{r\times F'}\) are two low-rank matrices such that \(r&lt;&lt;\min(F, F')\). Concretely, if \(F=512\), \(F'=512\), and \(|\mathcal{R}|=16\), then for all the \(W_r\) matrices, we would require \(16 * 512 * 512 = 4,194,304\) parameters per layer of the RGAT. However, using this proposed LoRA decomposition, if \(r = 5\), we only use \(512 * 512 + 16 * 2 * 512 * 5 = 344,064\), an order of magnitude reduction. As shown in the figure below, following the original LoRA paper, we use random Gaussian initialization for \(A_r\) and zero for \(B_r\).</p> <p><img src="/assets/img/lora.png" width="700px"></p> <h2 id="pooling-layer">Pooling Layer</h2> <p>The embeddings of all the nodes in the protein graph are aggregated to obtain a graph-level protein representation. We do so with a global mean pooling layer: \(\mathbf{p}=\frac{1}{N}\sum_{i=1}^N\mathbf{h}_i' \in\mathbb{R}^H\) where \(\mathbf{h}_i'\) is the final layer embedding for atom \(i\).</p> <h2 id="multi-task-learning-for-protein-functions">Multi-task Learning for Protein Functions</h2> <p>At test time, for each protein \(b\), the objective is to predict whether it possesses a randomly sampled function - which we call a \(\textit{task}\). This is a multi-task problem, whereby we hold out proteins during training and, at test time, we perform \(3\)-way classifications for all tasks associated with a protein. Specifically, for each protein \(b\) and atom \(i\), we feed input coordinates \(\mathbf{x}_i\) and features \(\mathbf{h}_i\) to a neural network \(f_{\theta}\) to obtain a \(D\)-dimensional latent protein representation \(\mathbf{p}_{\theta}^{(b)}\). Our dataset consists of \(B\) proteins, with protein embeddings \(\mathbf{p}_{\theta}^{(b)}\), and the corresponding ground-truth label for a task index \(t^{(b)}\). We define a positive dataset, consisting of pairs of a protein embedding and the corresponding labels for the function tasks associated with that protein:</p> \[\mathcal{D}^+:=\left\{\mathcal{D}_b^+\right\}_{b=1}^B:=\left\{\{(\mathbf{p}_{\theta}^{(b)}, y^{(t^{(b)})})\}_{t^{(b)}\in\mathcal{T}_b^+}\right\}_{b=1}^B\] <p>where \(\mathcal{T}_b^{+}\) is the set of all indices of real tasks for protein \(b\). We also define a negative dataset, consisting of pairs of a protein embedding and the labels for randomly sampled “negative” function tasks that we generated to balance the positive dataset:</p> \[\mathcal{D}^-:=\left\{\mathcal{D}_b^-\right\}_{b=1}^B:=\left\{\{(\mathbf{p}_{\theta}^{(b)}, y^{(t^{(b)})})\}_{t^{(b)}\in\mathcal{T}_b^-}\right\}_{b=1}^B\] <p>where \(\mathcal{T}_b^{-}\) denotes indices corresponding to the fake tasks for protein \(b\). Thus, for a protein \(b\), we sample a task \(t^{(b)}\) and compute its embedding with a look-up embedding table:</p> \[\mathbf{t}^{(b)}=\text{Embed}_{\psi}(t^{(b)})\in\mathbb{R}^{E}\] <p>We also obtain its latent protein embedding from its features and coordinates:</p> \[\mathbf{p}_{\theta}^{(b)}=f_{\theta}(H^{(b)},X^{(b)})\in\mathbb{R}^{H}\] <p>Finally, we use an MLP with a softmax applied to its logits to condition our classification on the task embedding and obtain the discrete probability distribution that protein \(b\) contributes to, enables or does not enable/contribute to protein function with task index \(t^{(b)}\):</p> \[\begin{split} &amp;g_{\phi}:\mathbb{R}^{E}\times \mathbb{R}^{H}\to[0,1]^3\\ &amp;\hat{y}^{(t^{(b)})}=g_{\phi}(\mathbf{t}^{(b)}, \mathbf{p}^{(b)}). \end{split}\] <p>Hence, we will minimize the following cross-entropy objective:</p> \[\min_{\psi,\theta,\phi}\sum_{b=1}^B \sum_{\left(\mathbf{p}_{\theta}^{(b)}, y^{(t^{(b)})}\right)\in\mathcal{D}_b^+\cup\mathcal{D}_b^-}\mathcal{H}\left(y^{(t^{(b)})}, g_{\phi}\Big(\text{Embed}_{\psi}(t^{(b)}), \mathbf{p}_{\theta}^{(b)}\Big)\right)\] <p>via a finite-sample approximation using mini-batch stochastic gradient descent. The complete architecture for FuncE GNN is illustrated below.</p> <p><img src="/assets/img/FuncEGNN.png" width="700px"></p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2026 Sergio Charles. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>